"""
ç¬¬ä¸€æ­¥ï¼šè·å–äº¤æ˜“ç»“æœé“¾æ¥åˆ—è¡¨
===============================
æœ¬è„šæœ¬ä½¿ç”¨ Playwright æ¨¡æ‹Ÿæµè§ˆå™¨æ“ä½œï¼Œç»•è¿‡ JSL åçˆ¬ä¿æŠ¤ï¼Œ
é€šè¿‡é¡µé¢ UI è®¾ç½®ç­›é€‰æ¡ä»¶åï¼Œæ‹¦æˆª API è¯·æ±‚ï¼Œç¿»é¡µè·å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„é“¾æ¥ã€‚

ç”¨æ³•ç¤ºä¾‹:
    # ä½¿ç”¨é»˜è®¤é…ç½® (å…³é”®è¯="", åŒºåŸŸ=å…¨éƒ¨, ä¸šåŠ¡=å…¨éƒ¨, ä¿¡æ¯=äº¤æ˜“ç»“æœ, æ—¶é—´=è¿‘ä¸‰æœˆ)
    python step1_fetch_links.py

    # è‡ªå®šä¹‰ç­›é€‰
    python step1_fetch_links.py -k "å‡ºç§Ÿ" -r "æ¸åŒ—åŒº" -t "è¿‘ä¸€æœˆ" -b "ä¸­ä»‹è¶…å¸‚" -i "æ‹›é€‰å…¬å‘Š"

å‚æ•°è¯´æ˜:
    -k, --keyword       æ ‡é¢˜å…³é”®è¯ (é»˜è®¤: "")
    -r, --region        è¡Œæ”¿åŒºåŸŸ (é»˜è®¤: "")
    -b, --biz-type      ä¸šåŠ¡ç±»å‹ (é»˜è®¤: "")
    -i, --info-type     ä¿¡æ¯ç±»å‹ (é»˜è®¤: "äº¤æ˜“ç»“æœ")
    -t, --time-period   å‘å¸ƒæ—¶é—´ (é»˜è®¤: "è¿‘ä¸‰æœˆ")

è¾“å‡º:
    output/links.json
"""

import argparse
import json
import os
import time
from datetime import datetime, timedelta
from playwright.sync_api import sync_playwright

# ============ é…ç½® ============
API_URL = "https://www.cqggzy.com/inteligentsearch/rest/esinteligentsearch/getFullTextDataNew"
PAGE_URL = "https://www.cqggzy.com/jyxx/transaction_detail.html"
BASE_URL = "https://www.cqggzy.com"
OUTPUT_DIR = "output"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "links.json")
DEFAULT_KEYWORD = ""
DEFAULT_REGION = ""  # é»˜è®¤æ— åŒºåŸŸé™åˆ¶
DEFAULT_BIZ_TYPE = ""  # é»˜è®¤æ— ä¸šåŠ¡ç±»å‹é™åˆ¶
DEFAULT_INFO_TYPE = "äº¤æ˜“ç»“æœ"  # é»˜è®¤ä¿¡æ¯ç±»å‹
DEFAULT_TIME_PERIOD = "è¿‘ä¸‰æœˆ"  # é»˜è®¤æ—¶é—´èŒƒå›´
PAGE_SIZE = 20  # æ¯é¡µæ¡æ•°
# ==============================


def get_time_range():
    now = datetime.now()
    start = now - timedelta(days=90)
    return start.strftime("%Y-%m-%d 00:00:00"), now.strftime("%Y-%m-%d 23:59:59")


def parse_records(api_data: dict) -> tuple[list[dict], int]:
    if api_data.get("code") != 200:
        print(f"  âŒ API é”™è¯¯: code={api_data.get('code')} msg={api_data.get('msg', '')}")
        return [], 0
    content = api_data.get("content", "")
    if isinstance(content, str):
        content = json.loads(content)
    result = content.get("result", {})
    return result.get("records", []), result.get("totalcount", 0)


def clean_record(record: dict) -> dict:
    link = record.get("linkurl", "")
    return {
        "æ ‡é¢˜": record.get("title", "").strip(),
        "å‘å¸ƒæ—¥æœŸ": record.get("pubinwebdate", ""),
        "ä¸šåŠ¡ç±»å‹": record.get("categorytype", ""),
        "åŒºåŸŸ": record.get("infoc", ""),
        "è®°å½•ID": record.get("newid", ""),
        "è¯¦æƒ…é“¾æ¥": f"{BASE_URL}{link}" if link else "",
    }


def pass_jsl(page) -> bool:
    """é€šè¿‡ JSL åçˆ¬éªŒè¯"""
    print("ğŸ”‘ é€šè¿‡ JSL åçˆ¬éªŒè¯...")
    page.goto(PAGE_URL, wait_until="commit", timeout=30000)
    for i in range(15):
        time.sleep(2)
        try:
            title = page.title()
            if "å…¬å…±èµ„æº" in title or "äº¤æ˜“" in title:
                print(f"  âœ… éªŒè¯é€šè¿‡! ({(i + 1) * 2}s)")
                return True
        except Exception:
            continue
    print("  âš  éªŒè¯è¶…æ—¶")
    return False


FETCH_SCRIPT = """
async (params) => {
    const resp = await fetch(params.url, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(params.body),
    });
    return await resp.json();
}
"""



def smart_click(page, text, timeout=2000):
    """å°è¯•ç‚¹å‡»æ–‡æœ¬å¯¹åº”çš„å…ƒç´ """
    if not text:
        return
    try:
        # é’ˆå¯¹ä¸åŒç±»å‹çš„ç­›é€‰é¡¹å°è¯•å®šä½
        # 1. å°è¯•ç›´æ¥æ–‡æœ¬åŒ¹é…çš„ label-item
        loc = page.locator(f"a.label-item:has-text('{text}')").first
        if loc.count() > 0:
            if "active" not in (loc.get_attribute("class") or ""):
                loc.click()
                time.sleep(1.5)
            print(f"  âœ… å·²é€‰ä¸­: {text}")
            return
        
        # 2. å°è¯•åŒ…å«æ–‡æœ¬çš„é“¾æ¥
        loc = page.locator(f"a:has-text('{text}')").first
        if loc.count() > 0:
            loc.click()
            time.sleep(1.5)
            print(f"  âœ… å·²ç‚¹å‡»: {text}")
            return

        print(f"  âš  æœªæ‰¾åˆ°ç­›é€‰é¡¹: {text}")
    except Exception as e:
        print(f"  âŒ ç‚¹å‡»å¤±è´¥ {text}: {e}")


def main():
    parser = argparse.ArgumentParser(description="æŠ“å–äº¤æ˜“ç»“æœé“¾æ¥")
    parser.add_argument("-k", "--keyword", default=DEFAULT_KEYWORD, help="æ ‡é¢˜å…³é”®è¯ (é»˜è®¤: å…¨éƒ¨)")
    parser.add_argument("-r", "--region", default=DEFAULT_REGION, help="è¡Œæ”¿åŒºåŸŸ (é»˜è®¤: å…¨éƒ¨)")
    parser.add_argument("-b", "--biz-type", default=DEFAULT_BIZ_TYPE, help="ä¸šåŠ¡ç±»å‹ (é»˜è®¤: å…¨éƒ¨)")
    parser.add_argument("-i", "--info-type", default=DEFAULT_INFO_TYPE, help=f"ä¿¡æ¯ç±»å‹ (é»˜è®¤: {DEFAULT_INFO_TYPE})")
    parser.add_argument("-t", "--time-period", default=DEFAULT_TIME_PERIOD, help=f"å‘å¸ƒæ—¶é—´ (é»˜è®¤: {DEFAULT_TIME_PERIOD})")

    args = parser.parse_args()

    keyword = args.keyword.strip()
    region = args.region.strip()
    biz_type = args.biz_type.strip()
    info_type = args.info_type.strip()
    time_period = args.time_period.strip()

    s, e = get_time_range()
    print("=" * 60)
    print("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šè·å–äº¤æ˜“ç»“æœé“¾æ¥åˆ—è¡¨")
    print("=" * 60)
    print(f"å…³é”®è¯: {keyword if keyword else 'å…¨éƒ¨'}")
    print(f"åŒºåŸŸ: {region if region else 'å…¨éƒ¨'} | ä¸šåŠ¡: {biz_type if biz_type else 'å…¨éƒ¨'}")
    print(f"ä¿¡æ¯: {info_type} | æ—¶é—´: {time_period}")
    print(f"æ—¶é—´èŒƒå›´: {s[:10]} ~ {e[:10]}")
    print("=" * 60)

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=False,
            args=["--disable-blink-features=AutomationControlled"],
        )
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            locale="zh-CN",
            viewport={"width": 1280, "height": 800},
        )
        page = context.new_page()

        # 1. é€šè¿‡ JSL
        if not pass_jsl(page):
            browser.close()
            return

        # 2. æ‹¦æˆª API è·å–æ­£ç¡®çš„è¯·æ±‚ä½“
        print("\nğŸ”§ è®¾ç½®ç­›é€‰æ¡ä»¶...")
        captured_body = None

        def on_response(response):
            nonlocal captured_body
            if API_URL in response.url:
                try:
                    captured_body = json.loads(response.request.post_data)
                except Exception:
                    pass

        page.on("response", on_response)

        # ç‚¹å‡»ä¿¡æ¯ç±»å‹ (å¦‚: äº¤æ˜“ç»“æœ)
        if info_type:
            smart_click(page, info_type)

        # ç‚¹å‡»å‘å¸ƒæ—¶é—´ (å¦‚: è¿‘ä¸‰æœˆ)
        if time_period:
            smart_click(page, time_period)
            
        # ç‚¹å‡»è¡Œæ”¿åŒºåŸŸ
        if region:
            smart_click(page, region)

        # ç‚¹å‡»ä¸šåŠ¡ç±»å‹
        if biz_type:
            smart_click(page, biz_type)

        # è¾“å…¥å…³é”®è¯æœç´¢
        try:
            inp = page.locator("input#search, input.input-box").first
            inp.clear()
            inp.fill(keyword)
            inp.press("Enter")
            time.sleep(3)
            print(f"  âœ… å·²æœç´¢: {keyword}")
        except Exception:
            print("  âš  æœç´¢æ¡†æœªæ‰¾åˆ°")

        time.sleep(2)

        # 3. ä½¿ç”¨æ‹¦æˆªåˆ°çš„è¯·æ±‚ä½“ç¿»é¡µè·å–æ•°æ®
        if not captured_body:
            print("  âŒ æœªèƒ½æ‹¦æˆªåˆ° API è¯·æ±‚ä½“")
            browser.close()
            return

        print(f"\nğŸ“Š å¼€å§‹è·å–é“¾æ¥ (æ¯é¡µ {PAGE_SIZE} æ¡)...")
        captured_body["pn"] = 0
        captured_body["rn"] = PAGE_SIZE

        resp = page.evaluate(FETCH_SCRIPT, {"url": API_URL, "body": captured_body})
        records, total = parse_records(resp)

        if not records:
            print("  âŒ æ— æ•°æ®")
            browser.close()
            return

        all_records = list(records)
        total_pages = (total + PAGE_SIZE - 1) // PAGE_SIZE
        print(f"  âœ… æ€»è®¡ {total} æ¡, {total_pages} é¡µ")

        for pn in range(1, total_pages):
            captured_body["pn"] = pn
            try:
                resp = page.evaluate(FETCH_SCRIPT, {"url": API_URL, "body": captured_body})
                recs, _ = parse_records(resp)
                all_records.extend(recs)
                if (pn + 1) % 10 == 0 or pn == total_pages - 1:
                    print(f"    {pn + 1}/{total_pages} é¡µ (ç´¯è®¡ {len(all_records)} æ¡)")
            except Exception as ex:
                print(f"    âŒ ç¬¬{pn + 1}é¡µ: {ex}")
                break
            time.sleep(0.3)

        browser.close()

    # æ¸…æ´—å¹¶ä¿å­˜
    links = [clean_record(r) for r in all_records]

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(links, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ å·²ä¿å­˜ {len(links)} æ¡é“¾æ¥ â†’ {OUTPUT_FILE}")
    cats = {}
    for r in links:
        k = r["ä¸šåŠ¡ç±»å‹"] or "æœªçŸ¥"
        cats[k] = cats.get(k, 0) + 1
    for k, v in sorted(cats.items(), key=lambda x: -x[1]):
        print(f"    {k}: {v} æ¡")
    print(f"\n{'=' * 60}")
    print(f"âœ… ç¬¬ä¸€æ­¥å®Œæˆ! æ¥ä¸‹æ¥è¿è¡Œ: python step2_scrape_details.py")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    main()
