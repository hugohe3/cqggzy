"""
ç¬¬ä¸€æ­¥ï¼šè·å–äº¤æ˜“ç»“æœé“¾æ¥åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
=========================================
æ¶æ„ï¼šæµè§ˆå™¨å¼€é—¨ + HTTP å¹²æ´»

Phase 1: ç”¨ Playwright é€šè¿‡ JSL éªŒè¯ï¼Œè®¾ç½®ç­›é€‰æ¡ä»¶ï¼Œæ‹¦æˆª API è¯·æ±‚ä½“ï¼Œæ‹¿åˆ° Cookie
Phase 2: ç«‹å³å…³é—­æµè§ˆå™¨ï¼Œç”¨ httpx é«˜é€Ÿç¿»é¡µè·å–æ‰€æœ‰é“¾æ¥

å¯¹æ¯”åŸç‰ˆ:
  - æµè§ˆå™¨ä»…ç”¨äº JSL éªŒè¯å’Œæ‹¦æˆªè¯·æ±‚ä½“ï¼Œç¿»é¡µå…¨èµ° HTTP
  - ç¿»é¡µé€Ÿåº¦æå‡ ~3xï¼ˆæ— éœ€æµè§ˆå™¨æ¸²æŸ“å¼€é”€ï¼‰
  - Cookie è‡ªåŠ¨ä¿å­˜ï¼Œä¾› step2 å¤ç”¨

ç”¨æ³•:
    python step1_fetch_links.py
    python step1_fetch_links.py -k "å‡ºç§Ÿ" -r "æ¸åŒ—åŒº" -t "è¿‘ä¸€æœˆ"

å‚æ•°:
    -k, --keyword       æ ‡é¢˜å…³é”®è¯ (é»˜è®¤: "")
    -r, --region        è¡Œæ”¿åŒºåŸŸ (é»˜è®¤: "")
    -b, --biz-type      ä¸šåŠ¡ç±»å‹ (é»˜è®¤: "")
    -i, --info-type     ä¿¡æ¯ç±»å‹ (é»˜è®¤: "äº¤æ˜“ç»“æœ")
    -t, --time-period   å‘å¸ƒæ—¶é—´ (é»˜è®¤: "è¿‘ä¸‰æœˆ")

è¾“å‡º:
    output/links.json
    output/cookies.json  (ä¾› step2 å¤ç”¨)
"""

import argparse
import json
import os
import time

import httpx
from playwright.sync_api import sync_playwright

from common.config import (
    API_URL, PAGE_URL, OUTPUT_DIR, LINKS_FILE,
    DEFAULT_KEYWORD, DEFAULT_REGION, DEFAULT_BIZ_TYPE,
    DEFAULT_INFO_TYPE, DEFAULT_TIME_PERIOD,
    PAGE_SIZE, USER_AGENT, REQUEST_TIMEOUT, MAX_PAGE_RETRIES,
)
from common.browser import (
    pass_jsl, create_browser_context,
    extract_cookies, save_cookies, smart_click,
)
from common.parser import parse_api_records, clean_record


def extract_request_body(response) -> dict | None:
    """ä» Playwright å“åº”å¯¹è±¡ä¸­æå– POST JSON è¯·æ±‚ä½“ã€‚"""
    if response.request.method != "POST":
        return None
    raw = response.request.post_data
    if not raw:
        return None
    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        return None


def main():
    # ========== å‚æ•°è§£æ ==========
    parser = argparse.ArgumentParser(description="æŠ“å–äº¤æ˜“ç»“æœé“¾æ¥")
    parser.add_argument("-k", "--keyword", default=DEFAULT_KEYWORD, help="æ ‡é¢˜å…³é”®è¯ (é»˜è®¤: å…¨éƒ¨)")
    parser.add_argument("-r", "--region", default=DEFAULT_REGION, help="è¡Œæ”¿åŒºåŸŸ (é»˜è®¤: å…¨éƒ¨)")
    parser.add_argument("-b", "--biz-type", default=DEFAULT_BIZ_TYPE, help="ä¸šåŠ¡ç±»å‹ (é»˜è®¤: å…¨éƒ¨)")
    parser.add_argument("-i", "--info-type", default=DEFAULT_INFO_TYPE, help=f"ä¿¡æ¯ç±»å‹ (é»˜è®¤: {DEFAULT_INFO_TYPE})")
    parser.add_argument("-t", "--time-period", default=DEFAULT_TIME_PERIOD, help=f"å‘å¸ƒæ—¶é—´ (é»˜è®¤: {DEFAULT_TIME_PERIOD})")
    args = parser.parse_args()

    keyword = args.keyword.strip()
    region = args.region.strip()
    biz_type = args.biz_type.strip()
    info_type = args.info_type.strip()
    time_period = args.time_period.strip()

    print("=" * 60)
    print("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šè·å–äº¤æ˜“ç»“æœé“¾æ¥åˆ—è¡¨")
    print("=" * 60)
    print(f"å…³é”®è¯: {keyword or 'å…¨éƒ¨'}")
    print(f"åŒºåŸŸ: {region or 'å…¨éƒ¨'} | ä¸šåŠ¡: {biz_type or 'å…¨éƒ¨'}")
    print(f"ä¿¡æ¯: {info_type} | æ—¶é—´: {time_period}")
    print("=" * 60)

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ===================================================================
    #  Phase 1: æµè§ˆå™¨ â€” è¿‡ JSL + è®¾ç½®ç­›é€‰ + æ‹¦æˆªè¯·æ±‚ä½“ + æ‹¿ Cookie
    # ===================================================================
    print("\nğŸŒ Phase 1: æµè§ˆå™¨è·å–éªŒè¯ Cookie å’Œ API è¯·æ±‚ä½“...")

    captured_body = None
    cookies = None

    with sync_playwright() as p:
        browser, context = create_browser_context(p)
        page = context.new_page()

        # 1) é€šè¿‡ JSL åçˆ¬éªŒè¯
        if not pass_jsl(page):
            browser.close()
            return

        # 2) æ‹¦æˆª API è¯·æ±‚ä½“
        def on_response(response):
            nonlocal captured_body
            if API_URL in response.url:
                parsed = extract_request_body(response)
                if parsed:
                    captured_body = parsed

        page.on("response", on_response)

        # 3) é€šè¿‡ UI è®¾ç½®ç­›é€‰æ¡ä»¶ï¼ˆè§¦å‘ç¬¬ä¸€æ¬¡ API è¯·æ±‚ï¼‰
        print("\nğŸ”§ è®¾ç½®ç­›é€‰æ¡ä»¶...")
        if info_type:
            smart_click(page, info_type)
        if time_period:
            smart_click(page, time_period)
        if region:
            smart_click(page, region)
        if biz_type:
            smart_click(page, biz_type)

        # 4) è¾“å…¥å…³é”®è¯æœç´¢ï¼Œå¹¶æ˜¾å¼ç­‰å¾…ç›®æ ‡ API å“åº”
        try:
            inp = page.locator("input#search, input.input-box").first
            inp.clear()
            inp.fill(keyword)
            try:
                with page.expect_response(
                    lambda r: API_URL in r.url and r.request.method == "POST",
                    timeout=15000,
                ) as resp_info:
                    inp.press("Enter")
                parsed = extract_request_body(resp_info.value)
                if parsed:
                    captured_body = parsed
                print(f"  âœ… å·²æœç´¢: {keyword}")
            except Exception:
                # å…œåº•ï¼šå¦‚æœ expect_response è¶…æ—¶ï¼Œç»§ç»­ä¾èµ–è¢«åŠ¨æ‹¦æˆªç»“æœ
                inp.press("Enter")
                print(f"  âš  æœç´¢å·²è§¦å‘ï¼Œä½†æœªåœ¨è¶…æ—¶æ—¶é—´å†…æ•è·åˆ°å“åº”: {keyword}")
        except Exception:
            print("  âš  æœç´¢æ¡†æœªæ‰¾åˆ°")

        # æœç´¢æ¡†ä¸å¯ç”¨æ—¶ï¼Œå°è¯•å†ä¸»åŠ¨ç­‰å¾…ä¸€æ¬¡å“åº”ï¼Œå‡å°‘æ—¶åºç«æ€
        if not captured_body:
            try:
                resp = page.wait_for_response(
                    lambda r: API_URL in r.url and r.request.method == "POST",
                    timeout=8000,
                )
                parsed = extract_request_body(resp)
                if parsed:
                    captured_body = parsed
            except Exception:
                pass

        if not captured_body:
            print("  âŒ æœªèƒ½æ‹¦æˆªåˆ° API è¯·æ±‚ä½“ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ")
            browser.close()
            return

        # 5) æå– Cookieï¼Œç«‹å³å…³é—­æµè§ˆå™¨
        cookies = extract_cookies(context)
        save_cookies(cookies)
        browser.close()
        print("  âœ… å·²è·å– Cookieï¼Œæµè§ˆå™¨å·²å…³é—­\n")

    # ===================================================================
    #  Phase 2: HTTP å¿«é€Ÿç¿»é¡µ
    # ===================================================================
    print(f"ğŸš€ Phase 2: HTTP å¿«é€Ÿç¿»é¡µ (æ¯é¡µ {PAGE_SIZE} æ¡)...")

    captured_body["pn"] = 0
    captured_body["rn"] = PAGE_SIZE

    headers = {
        "Content-Type": "application/json",
        "X-Requested-With": "XMLHttpRequest",
        "User-Agent": USER_AGENT,
        "Referer": PAGE_URL,
    }

    with httpx.Client(cookies=cookies, headers=headers, timeout=REQUEST_TIMEOUT) as client:
        # é¦–é¡µ
        resp = client.post(API_URL, json=captured_body)
        records, total = parse_api_records(resp.json())

        if not records:
            print("  âŒ æ— æ•°æ®")
            return

        all_records = list(records)
        total_pages = (total + PAGE_SIZE - 1) // PAGE_SIZE
        print(f"  âœ… æ€»è®¡ {total} æ¡, {total_pages} é¡µ")

        # ç¿»é¡µï¼ˆå•é¡µå¤±è´¥é‡è¯•ï¼Œå¤±è´¥é¡µè·³è¿‡å¹¶ç»§ç»­åç»­é¡µï¼‰
        failed_pages = []
        for pn in range(1, total_pages):
            captured_body["pn"] = pn
            ok = False
            for attempt in range(1, MAX_PAGE_RETRIES + 1):
                try:
                    resp = client.post(API_URL, json=captured_body)
                    recs, _ = parse_api_records(resp.json())
                    all_records.extend(recs)
                    ok = True
                    break
                except Exception as ex:
                    if attempt < MAX_PAGE_RETRIES:
                        wait = 0.2 * attempt
                        print(f"    âš  ç¬¬{pn + 1}é¡µå¤±è´¥(ç¬¬{attempt}æ¬¡): {ex}ï¼Œ{wait:.1f}s åé‡è¯•")
                        time.sleep(wait)
                    else:
                        print(f"    âŒ ç¬¬{pn + 1}é¡µæœ€ç»ˆå¤±è´¥: {ex}")
            if not ok:
                failed_pages.append(pn + 1)
                continue
            if (pn + 1) % 10 == 0 or pn == total_pages - 1:
                print(f"    {pn + 1}/{total_pages} é¡µ (ç´¯è®¡ {len(all_records)} æ¡)")
            time.sleep(0.1)  # è½»é‡é—´éš”ï¼ŒHTTP æ— éœ€é•¿ç­‰å¾…

        if failed_pages:
            preview = ", ".join(str(x) for x in failed_pages[:10])
            more = "..." if len(failed_pages) > 10 else ""
            print(f"  âš  è·³è¿‡å¤±è´¥é¡µ {len(failed_pages)} ä¸ª: {preview}{more}")

    # ===================================================================
    #  ä¿å­˜ç»“æœ
    # ===================================================================
    links = [clean_record(r) for r in all_records]

    with open(LINKS_FILE, "w", encoding="utf-8") as f:
        json.dump(links, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ å·²ä¿å­˜ {len(links)} æ¡é“¾æ¥ â†’ {LINKS_FILE}")

    # ç»Ÿè®¡ä¸šåŠ¡ç±»å‹åˆ†å¸ƒ
    cats = {}
    for r in links:
        k = r["ä¸šåŠ¡ç±»å‹"] or "æœªçŸ¥"
        cats[k] = cats.get(k, 0) + 1
    for k, v in sorted(cats.items(), key=lambda x: -x[1]):
        print(f"    {k}: {v} æ¡")

    print(f"\n{'=' * 60}")
    print("âœ… ç¬¬ä¸€æ­¥å®Œæˆ! æ¥ä¸‹æ¥è¿è¡Œ: python step2_scrape_details.py")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    main()
