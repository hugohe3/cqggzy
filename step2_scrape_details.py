"""
ç¬¬äºŒæ­¥ï¼šæŠ“å–è¯¦æƒ…é¡µæ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
==================================
æ¶æ„ï¼šCookie å¤ç”¨ + å¼‚æ­¥å¹¶å‘ HTTP

æ ¸å¿ƒä¼˜åŒ–:
  1. å¤ç”¨ step1 ä¿å­˜çš„ Cookieï¼ˆè¿‡æœŸè‡ªåŠ¨é‡æ–°è·å–ï¼‰
  2. ç”¨ httpx AsyncClient å¼‚æ­¥å¹¶å‘æŠ“å–è¯¦æƒ…é¡µ
  3. ç”¨ BeautifulSoup è§£æ HTMLï¼ˆæ— éœ€æµè§ˆå™¨ï¼‰
  4. å¹¶å‘æ•°å¯æ§ï¼ŒæŒ‰æ‰¹ä¿å­˜è¿›åº¦ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 

å¯¹æ¯”åŸç‰ˆ:
  - é€Ÿåº¦æå‡ 10-50xï¼ˆasync å¹¶å‘ vs ä¸²è¡Œ Playwrightï¼‰
  - å†…å­˜å ç”¨é™ä½ ~90%ï¼ˆæ—  Chromium è¿›ç¨‹ï¼‰
  - åºå·è®¡ç®—ä¿®å¤ï¼ˆä½¿ç”¨ orig_idx + 1ï¼‰
  - progress.completed è‡ªåŠ¨å»é‡

ç”¨æ³•:
    python step2_scrape_details.py

è¾“å‡º:
    output/details.csv  (Excel å¯æ‰“å¼€)
    output/details.json (åŸå§‹æ•°æ®)
"""

import asyncio
import csv
import json
import os
import time

import httpx
from playwright.sync_api import sync_playwright

from common.config import (
    BASE_URL, OUTPUT_DIR, LINKS_FILE, DETAILS_CSV, DETAILS_JSON,
    PROGRESS_FILE, PAGE_SIZE, BATCH_SIZE,
    USER_AGENT, MAX_CONCURRENT, REQUEST_TIMEOUT,
)
from common.browser import (
    pass_jsl, create_browser_context,
    extract_cookies, save_cookies, load_cookies,
)
from common.parser import parse_detail_html


# ------------------------------------------------------------------
#  è¿›åº¦ç®¡ç†
# ------------------------------------------------------------------

def load_progress() -> dict:
    """åŠ è½½æ–­ç‚¹ç»­ä¼ è¿›åº¦"""
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        # è‡ªåŠ¨å»é‡ completed åˆ—è¡¨ï¼ˆä¿®å¤åŸç‰ˆçš„é‡å¤é—®é¢˜ï¼‰
        data["completed"] = list(set(data.get("completed", [])))
        return data
    return {"completed": [], "details": []}


def save_progress(progress: dict):
    """ä¿å­˜è¿›åº¦ï¼ˆè‡ªåŠ¨å»é‡ completedï¼‰"""
    progress["completed"] = list(set(progress["completed"]))
    with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)


# ------------------------------------------------------------------
#  CSV è¾“å‡º
# ------------------------------------------------------------------

def save_csv(details: list[dict]):
    """ä¿å­˜ä¸º UTF-8 BOM ç¼–ç çš„ CSVï¼ˆExcel å¯ç›´æ¥æ‰“å¼€ï¼‰"""
    if not details:
        return

    base = ["åºå·", "æ ‡é¢˜", "å‘å¸ƒæ—¥æœŸ", "ä¸šåŠ¡ç±»å‹", "åŒºåŸŸ", "è¯¦æƒ…é“¾æ¥"]
    extra = set()
    for d in details:
        for k in d.keys():
            if k not in base:
                extra.add(k)

    # æ’åºï¼šå¸¸è§„å­—æ®µ â†’ é”™è¯¯ â†’ æ­£æ–‡å†…å®¹ï¼ˆæ”¾æœ€åï¼‰
    extra_sorted = sorted(extra - {"æ­£æ–‡å†…å®¹", "é”™è¯¯"})
    if "é”™è¯¯" in extra:
        extra_sorted.append("é”™è¯¯")
    if "æ­£æ–‡å†…å®¹" in extra:
        extra_sorted.append("æ­£æ–‡å†…å®¹")
    all_fields = base + extra_sorted

    with open(DETAILS_CSV, "w", encoding="utf-8-sig", newline="") as f:
        w = csv.DictWriter(f, fieldnames=all_fields, extrasaction="ignore")
        w.writeheader()
        w.writerows(details)
    print(f"ğŸ’¾ CSV â†’ {DETAILS_CSV} ({len(details)} æ¡, {len(all_fields)} åˆ—)")


# ------------------------------------------------------------------
#  Cookie è·å–
# ------------------------------------------------------------------

def acquire_cookies() -> dict:
    """è·å–æœ‰æ•ˆçš„ JSL Cookie

    ç­–ç•¥:
      1. å…ˆå°è¯• step1 ä¿å­˜çš„ Cookie
      2. ç”¨ HTTP è¯·æ±‚éªŒè¯æœ‰æ•ˆæ€§
      3. æ— æ•ˆåˆ™é‡æ–°å¯åŠ¨æµè§ˆå™¨è·å–
    """
    saved = load_cookies()
    if saved:
        try:
            resp = httpx.get(
                f"{BASE_URL}/jyxx/transaction_detail.html",
                cookies=saved,
                headers={"User-Agent": USER_AGENT},
                timeout=10,
                follow_redirects=True,
            )
            if resp.status_code == 200 and ("å…¬å…±èµ„æº" in resp.text or "äº¤æ˜“" in resp.text):
                print("  âœ… å·²ä¿å­˜çš„ Cookie æœ‰æ•ˆ")
                return saved
        except Exception:
            pass
        print("  âš  Cookie å·²è¿‡æœŸ, é‡æ–°è·å–...")

    # é‡æ–°é€šè¿‡æµè§ˆå™¨è·å–
    with sync_playwright() as p:
        browser, context = create_browser_context(p)
        page = context.new_page()

        if not pass_jsl(page):
            browser.close()
            raise RuntimeError("æ— æ³•é€šè¿‡ JSL éªŒè¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ")

        cookies = extract_cookies(context)
        save_cookies(cookies)
        browser.close()

    return cookies


# ------------------------------------------------------------------
#  å¼‚æ­¥æŠ“å–æ ¸å¿ƒ
# ------------------------------------------------------------------

async def fetch_one(
    client: httpx.AsyncClient,
    url: str,
    semaphore: asyncio.Semaphore,
    retry: int = 2,
) -> dict:
    """å¼‚æ­¥æŠ“å–å•ä¸ªè¯¦æƒ…é¡µï¼ˆå¸¦é‡è¯•ï¼‰"""
    async with semaphore:
        for attempt in range(retry + 1):
            try:
                resp = await client.get(url, timeout=REQUEST_TIMEOUT, follow_redirects=True)
                if resp.status_code == 521:
                    # JSL challenge â€” Cookie å¯èƒ½å·²è¿‡æœŸï¼Œæ— æ³•åœ¨å¼‚æ­¥ä¸­æ¢å¤
                    return {"é”™è¯¯": "Cookie è¿‡æœŸ(521)"}
                if resp.status_code != 200:
                    if attempt < retry:
                        await asyncio.sleep(1)
                        continue
                    return {"é”™è¯¯": f"HTTP {resp.status_code}"}
                return parse_detail_html(resp.text)
            except (httpx.TimeoutException, httpx.ConnectError) as e:
                if attempt < retry:
                    await asyncio.sleep(1)
                    continue
                return {"é”™è¯¯": str(e)}
            except Exception as e:
                return {"é”™è¯¯": str(e)}
    return {"é”™è¯¯": "æœªçŸ¥é”™è¯¯"}


async def scrape_batch(
    pending: list[tuple[int, dict]],
    cookies: dict,
    all_details: list[dict],
    completed_ids: set,
    progress: dict,
) -> int:
    """å¼‚æ­¥æ‰¹é‡æŠ“å–ï¼ŒæŒ‰æ‰¹ä¿å­˜è¿›åº¦"""
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9",
    }

    total = len(pending)
    processed = 0
    errors = 0

    async with httpx.AsyncClient(cookies=cookies, headers=headers) as client:
        # æŒ‰ BATCH_SIZE åˆ†æ‰¹å¤„ç†
        for batch_start in range(0, total, BATCH_SIZE):
            batch = pending[batch_start : batch_start + BATCH_SIZE]

            # å¹¶å‘æŠ“å–æ•´æ‰¹
            tasks = [fetch_one(client, record["è¯¦æƒ…é“¾æ¥"], semaphore) for _, record in batch]
            results = await asyncio.gather(*tasks)

            # å¤„ç†ç»“æœ
            for (orig_idx, record), detail in zip(batch, results):
                rid = record["è®°å½•ID"]
                processed += 1

                merged = {
                    "åºå·": orig_idx + 1,  # ä¿®å¤ï¼šä½¿ç”¨åŸå§‹ç´¢å¼•
                    "æ ‡é¢˜": record["æ ‡é¢˜"],
                    "å‘å¸ƒæ—¥æœŸ": record["å‘å¸ƒæ—¥æœŸ"],
                    "ä¸šåŠ¡ç±»å‹": record["ä¸šåŠ¡ç±»å‹"],
                    "åŒºåŸŸ": record["åŒºåŸŸ"],
                    "è¯¦æƒ…é“¾æ¥": record["è¯¦æƒ…é“¾æ¥"],
                }
                merged.update(detail)
                all_details.append(merged)
                completed_ids.add(rid)
                progress["completed"].append(rid)

                has_error = "é”™è¯¯" in detail
                if has_error:
                    errors += 1

                title = record["æ ‡é¢˜"][:40]
                status = "âŒ" if has_error else "âœ…"
                keys = [k for k in detail if k not in ("æ­£æ–‡å†…å®¹", "é”™è¯¯")]
                info = detail.get("é”™è¯¯", f"{len(keys)} ä¸ªå­—æ®µ")
                print(f"  [{processed}/{total}] {status} {title}... ({info})")

            # æ¯æ‰¹ä¿å­˜ä¸€æ¬¡è¿›åº¦
            progress["details"] = all_details
            save_progress(progress)
            pct = processed / total * 100
            print(f"  ğŸ’¾ è¿›åº¦ {pct:.0f}% ({len(completed_ids)} æ¡å®Œæˆ)")

    return errors


# ------------------------------------------------------------------
#  ä¸»æµç¨‹
# ------------------------------------------------------------------

def main():
    print("=" * 60)
    print("ğŸ“„ ç¬¬äºŒæ­¥ï¼šæŠ“å–è¯¦æƒ…é¡µæ•°æ®")
    print("=" * 60)

    if not os.path.exists(LINKS_FILE):
        print(f"âŒ æœªæ‰¾åˆ° {LINKS_FILE}ï¼Œè¯·å…ˆè¿è¡Œ step1_fetch_links.py")
        return

    with open(LINKS_FILE, "r", encoding="utf-8") as f:
        records = json.load(f)
    print(f"ğŸ“„ åŠ è½½ {len(records)} æ¡é“¾æ¥")

    # åŠ è½½è¿›åº¦ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    progress = load_progress()
    completed_ids = set(progress["completed"])
    all_details = progress["details"]
    print(f"ğŸ“Š å·²å®Œæˆ: {len(completed_ids)} æ¡")

    pending = [(i, r) for i, r in enumerate(records) if r["è®°å½•ID"] not in completed_ids]
    print(f"â³ å¾…å¤„ç†: {len(pending)} æ¡")

    if not pending:
        print("âœ… å…¨éƒ¨å·²å®Œæˆ!")
        save_csv(all_details)
        return

    # è·å– Cookie
    print("\nğŸ”‘ è·å–éªŒè¯ Cookie...")
    cookies = acquire_cookies()

    # å¼‚æ­¥æŠ“å–
    print(f"\nğŸš€ å¼€å§‹å¼‚æ­¥æŠ“å– (å¹¶å‘æ•°: {MAX_CONCURRENT})...")
    start_time = time.time()

    errors = asyncio.run(
        scrape_batch(pending, cookies, all_details, completed_ids, progress)
    )

    elapsed = time.time() - start_time

    # æœ€ç»ˆä¿å­˜
    progress["details"] = all_details
    save_progress(progress)

    with open(DETAILS_JSON, "w", encoding="utf-8") as f:
        json.dump(all_details, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ JSON â†’ {DETAILS_JSON}")

    save_csv(all_details)

    # ç»Ÿè®¡
    cats = {}
    for c in all_details:
        k = c.get("ä¸šåŠ¡ç±»å‹") or "æœªçŸ¥"
        cats[k] = cats.get(k, 0) + 1
    print("\nğŸ“Š ä¸šåŠ¡ç±»å‹åˆ†å¸ƒ:")
    for k, v in sorted(cats.items(), key=lambda x: -x[1]):
        print(f"    {k}: {v} æ¡")

    print(f"\n{'=' * 60}")
    print(f"âœ… ç¬¬äºŒæ­¥å®Œæˆ! å…± {len(all_details)} æ¡è¯¦æƒ…")
    print(f"â±  è€—æ—¶: {elapsed:.1f}s | å¹³å‡: {elapsed / max(len(pending), 1):.2f}s/æ¡")
    if errors:
        print(f"âš   {errors} æ¡æŠ“å–å¤±è´¥ï¼ˆå¯é‡æ–°è¿è¡Œè‡ªåŠ¨é‡è¯•ï¼‰")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    main()
