"""
ç¬¬äºŒæ­¥ï¼šæŠ“å–è¯¦æƒ…é¡µæ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
==================================
æ¶æ„ï¼šCookie å¤ç”¨ + å¼‚æ­¥å¹¶å‘ HTTP

æ ¸å¿ƒä¼˜åŒ–:
  1. å¤ç”¨ step1 ä¿å­˜çš„ Cookieï¼ˆè¿‡æœŸè‡ªåŠ¨é‡æ–°è·å–ï¼‰
  2. ç”¨ httpx AsyncClient å¼‚æ­¥å¹¶å‘æŠ“å–è¯¦æƒ…é¡µ
  3. ç”¨ BeautifulSoup è§£æ HTMLï¼ˆæ— éœ€æµè§ˆå™¨ï¼‰
  4. å¹¶å‘æ•°å¯æ§ï¼ŒæŒ‰æ‰¹ä¿å­˜è¿›åº¦ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 

å¯¹æ¯”åŸç‰ˆ:
  - é€Ÿåº¦æå‡ 10-50xï¼ˆasync å¹¶å‘ vs ä¸²è¡Œ Playwrightï¼‰
  - å†…å­˜å ç”¨é™ä½ ~90%ï¼ˆæ—  Chromium è¿›ç¨‹ï¼‰
  - åºå·è®¡ç®—ä¿®å¤ï¼ˆä½¿ç”¨ orig_idx + 1ï¼‰
  - progress.completed è‡ªåŠ¨å»é‡

ç”¨æ³•:
    python step2_scrape_details.py

è¾“å‡º:
    output/details.csv  (Excel å¯æ‰“å¼€)
    output/details.json (åŸå§‹æ•°æ®)
"""

import asyncio
import csv
import json
import os
import time

import httpx
from playwright.sync_api import sync_playwright

from common.config import (
    BASE_URL, LINKS_FILE, DETAILS_CSV, DETAILS_JSON,
    PROGRESS_FILE, PROGRESS_SAVE_INTERVAL, DETAIL_RETRY,
    USER_AGENT, MAX_CONCURRENT, REQUEST_TIMEOUT,
)
from common.browser import (
    pass_jsl, create_browser_context,
    extract_cookies, save_cookies, load_cookies,
)
from common.parser import parse_detail_html


# ------------------------------------------------------------------
#  è¿›åº¦ç®¡ç†
# ------------------------------------------------------------------

def load_progress() -> dict:
    """åŠ è½½æ–­ç‚¹ç»­ä¼ è¿›åº¦"""
    default = {"completed": [], "failed": {}, "details": []}
    if os.path.exists(PROGRESS_FILE):
        try:
            with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
        except (json.JSONDecodeError, OSError) as e:
            print(f"  âš  è¿›åº¦æ–‡ä»¶æŸåæˆ–ä¸å¯è¯»ï¼Œå·²å¿½ç•¥: {e}")
            return default

        completed = data.get("completed", [])
        if not isinstance(completed, list):
            completed = []
        # ä¿åºå»é‡ï¼Œé¿å… set æ‰“ä¹±é¡ºåº
        completed = list(dict.fromkeys(completed))

        failed = data.get("failed", {})
        if not isinstance(failed, dict):
            failed = {}

        details = data.get("details", [])
        if not isinstance(details, list):
            details = []

        return {
            "completed": completed,
            "failed": failed,
            "details": details,
        }
    return default


def save_progress(progress: dict):
    """åŸå­ä¿å­˜è¿›åº¦ï¼Œé¿å…ä¸­æ–­æ—¶å†™åæ–‡ä»¶ã€‚"""
    failed = progress.get("failed", {})
    if not isinstance(failed, dict):
        failed = {}
    details = progress.get("details", [])
    if not isinstance(details, list):
        details = []

    normalized = {
        "completed": list(dict.fromkeys(progress.get("completed", []))),
        "failed": failed,
        "details": details,
    }
    tmp_file = f"{PROGRESS_FILE}.tmp"
    with open(tmp_file, "w", encoding="utf-8") as f:
        json.dump(normalized, f, ensure_ascii=False, indent=2)
    os.replace(tmp_file, PROGRESS_FILE)


# ------------------------------------------------------------------
#  CSV è¾“å‡º
# ------------------------------------------------------------------

def save_csv(details: list[dict]):
    """ä¿å­˜ä¸º UTF-8 BOM ç¼–ç çš„ CSVï¼ˆExcel å¯ç›´æ¥æ‰“å¼€ï¼‰"""
    if not details:
        return

    base = ["åºå·", "æ ‡é¢˜", "å‘å¸ƒæ—¥æœŸ", "ä¸šåŠ¡ç±»å‹", "åŒºåŸŸ", "è¯¦æƒ…é“¾æ¥"]
    extra = set()
    for d in details:
        for k in d.keys():
            if k not in base:
                extra.add(k)

    # æ’åºï¼šå¸¸è§„å­—æ®µ â†’ é”™è¯¯ â†’ æ­£æ–‡å†…å®¹ï¼ˆæ”¾æœ€åï¼‰
    extra_sorted = sorted(extra - {"æ­£æ–‡å†…å®¹", "é”™è¯¯"})
    if "é”™è¯¯" in extra:
        extra_sorted.append("é”™è¯¯")
    if "æ­£æ–‡å†…å®¹" in extra:
        extra_sorted.append("æ­£æ–‡å†…å®¹")
    all_fields = base + extra_sorted

    with open(DETAILS_CSV, "w", encoding="utf-8-sig", newline="") as f:
        w = csv.DictWriter(f, fieldnames=all_fields, extrasaction="ignore")
        w.writeheader()
        w.writerows(details)
    print(f"ğŸ’¾ CSV â†’ {DETAILS_CSV} ({len(details)} æ¡, {len(all_fields)} åˆ—)")


# ------------------------------------------------------------------
#  Cookie è·å–
# ------------------------------------------------------------------

def acquire_cookies() -> dict:
    """è·å–æœ‰æ•ˆçš„ JSL Cookie

    ç­–ç•¥:
      1. å…ˆå°è¯• step1 ä¿å­˜çš„ Cookie
      2. ç”¨ HTTP è¯·æ±‚éªŒè¯æœ‰æ•ˆæ€§
      3. æ— æ•ˆåˆ™é‡æ–°å¯åŠ¨æµè§ˆå™¨è·å–
    """
    saved = load_cookies()
    if saved:
        try:
            resp = httpx.get(
                f"{BASE_URL}/jyxx/transaction_detail.html",
                cookies=saved,
                headers={"User-Agent": USER_AGENT},
                timeout=10,
                follow_redirects=True,
            )
            if resp.status_code == 200 and ("å…¬å…±èµ„æº" in resp.text or "äº¤æ˜“" in resp.text):
                print("  âœ… å·²ä¿å­˜çš„ Cookie æœ‰æ•ˆ")
                return saved
        except Exception:
            pass
        print("  âš  Cookie å·²è¿‡æœŸ, é‡æ–°è·å–...")

    # é‡æ–°é€šè¿‡æµè§ˆå™¨è·å–
    with sync_playwright() as p:
        browser, context = create_browser_context(p)
        page = context.new_page()

        if not pass_jsl(page):
            browser.close()
            raise RuntimeError("æ— æ³•é€šè¿‡ JSL éªŒè¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ")

        cookies = extract_cookies(context)
        save_cookies(cookies)
        browser.close()

    return cookies


# ------------------------------------------------------------------
#  å¼‚æ­¥æŠ“å–æ ¸å¿ƒ
# ------------------------------------------------------------------

async def fetch_one(
    client: httpx.AsyncClient,
    url: str,
    retry: int = DETAIL_RETRY,
) -> dict:
    """å¼‚æ­¥æŠ“å–å•ä¸ªè¯¦æƒ…é¡µï¼ˆå¸¦é‡è¯•ï¼‰"""
    for attempt in range(retry + 1):
        try:
            resp = await client.get(url, timeout=REQUEST_TIMEOUT, follow_redirects=True)
            if resp.status_code == 521:
                # JSL challenge â€” Cookie å¯èƒ½å·²è¿‡æœŸï¼Œæ— æ³•åœ¨å¼‚æ­¥ä¸­æ¢å¤
                return {"é”™è¯¯": "Cookie è¿‡æœŸ(521)"}
            if resp.status_code != 200:
                if attempt < retry:
                    await asyncio.sleep(1)
                    continue
                return {"é”™è¯¯": f"HTTP {resp.status_code}"}
            return parse_detail_html(resp.text)
        except (httpx.TimeoutException, httpx.ConnectError) as e:
            if attempt < retry:
                await asyncio.sleep(1)
                continue
            return {"é”™è¯¯": str(e)}
        except Exception as e:
            return {"é”™è¯¯": str(e)}
    return {"é”™è¯¯": "æœªçŸ¥é”™è¯¯"}


def sort_details(details: list[dict]) -> list[dict]:
    """æŒ‰åºå·æ’åºï¼Œç¡®ä¿è¾“å‡ºé¡ºåºç¨³å®šã€‚"""
    return sorted(details, key=lambda x: x.get("åºå·", 10**9))


def apply_scrape_result(
    orig_idx: int,
    record: dict,
    detail: dict,
    all_details: list[dict],
    detail_index: dict,
    completed_ids: set,
    progress: dict,
) -> bool:
    """åˆå¹¶å•æ¡æŠ“å–ç»“æœï¼Œè¿”å›æ˜¯å¦å¤±è´¥ã€‚"""
    progress.setdefault("completed", [])
    progress.setdefault("failed", {})
    rid = record["è®°å½•ID"]
    has_error = "é”™è¯¯" in detail

    if has_error:
        progress["failed"][rid] = detail.get("é”™è¯¯", "æœªçŸ¥é”™è¯¯")
        return True

    merged = {
        "åºå·": orig_idx + 1,
        "æ ‡é¢˜": record["æ ‡é¢˜"],
        "å‘å¸ƒæ—¥æœŸ": record["å‘å¸ƒæ—¥æœŸ"],
        "ä¸šåŠ¡ç±»å‹": record["ä¸šåŠ¡ç±»å‹"],
        "åŒºåŸŸ": record["åŒºåŸŸ"],
        "è¯¦æƒ…é“¾æ¥": record["è¯¦æƒ…é“¾æ¥"],
    }
    merged.update(detail)

    key = record["è¯¦æƒ…é“¾æ¥"] or rid
    if key in detail_index:
        all_details[detail_index[key]] = merged
    else:
        detail_index[key] = len(all_details)
        all_details.append(merged)

    completed_ids.add(rid)
    progress["completed"].append(rid)
    progress["failed"].pop(rid, None)
    return False


async def scrape_batch(
    pending: list[tuple[int, dict]],
    cookies: dict,
    all_details: list[dict],
    completed_ids: set,
    progress: dict,
) -> int:
    """å¼‚æ­¥æŠ“å–ï¼Œè§£è€¦å¹¶å‘ä¸Šé™ä¸è¿›åº¦ä¿å­˜é¢‘ç‡ã€‚"""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9",
    }

    total = len(pending)
    processed = 0
    errors = 0
    save_interval = max(PROGRESS_SAVE_INTERVAL, 1)
    detail_index = {
        (d.get("è¯¦æƒ…é“¾æ¥") or f"idx:{i}"): i for i, d in enumerate(all_details)
    }
    queue: asyncio.Queue[tuple[int, dict]] = asyncio.Queue()
    for item in pending:
        queue.put_nowait(item)

    async with httpx.AsyncClient(cookies=cookies, headers=headers) as client:
        async def worker():
            nonlocal processed, errors
            while True:
                try:
                    orig_idx, record = queue.get_nowait()
                except asyncio.QueueEmpty:
                    return

                try:
                    detail = await fetch_one(client, record["è¯¦æƒ…é“¾æ¥"])
                    has_error = apply_scrape_result(
                        orig_idx,
                        record,
                        detail,
                        all_details,
                        detail_index,
                        completed_ids,
                        progress,
                    )
                    processed += 1
                    if has_error:
                        errors += 1

                    title = record["æ ‡é¢˜"][:40]
                    status = "âŒ" if has_error else "âœ…"
                    keys = [k for k in detail if k not in ("æ­£æ–‡å†…å®¹", "é”™è¯¯")]
                    info = detail.get("é”™è¯¯", f"{len(keys)} ä¸ªå­—æ®µ")
                    print(f"  [{processed}/{total}] {status} {title}... ({info})")

                    if processed % save_interval == 0 or processed == total:
                        progress["details"] = sort_details(all_details)
                        save_progress(progress)
                        pct = processed / total * 100
                        print(f"  ğŸ’¾ è¿›åº¦ {pct:.0f}% ({len(completed_ids)} æ¡å®Œæˆ)")
                finally:
                    queue.task_done()

        worker_count = max(1, min(MAX_CONCURRENT, total))
        workers = [asyncio.create_task(worker()) for _ in range(worker_count)]
        await queue.join()
        for task in workers:
            task.cancel()
        await asyncio.gather(*workers, return_exceptions=True)

    return errors


# ------------------------------------------------------------------
#  ä¸»æµç¨‹
# ------------------------------------------------------------------

def main():
    print("=" * 60)
    print("ğŸ“„ ç¬¬äºŒæ­¥ï¼šæŠ“å–è¯¦æƒ…é¡µæ•°æ®")
    print("=" * 60)

    if not os.path.exists(LINKS_FILE):
        print(f"âŒ æœªæ‰¾åˆ° {LINKS_FILE}ï¼Œè¯·å…ˆè¿è¡Œ step1_fetch_links.py")
        return

    with open(LINKS_FILE, "r", encoding="utf-8") as f:
        records = json.load(f)
    print(f"ğŸ“„ åŠ è½½ {len(records)} æ¡é“¾æ¥")

    # åŠ è½½è¿›åº¦ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    progress = load_progress()
    completed_ids = set(progress["completed"])
    all_details = sort_details(progress["details"])
    print(f"ğŸ“Š å·²å®Œæˆ: {len(completed_ids)} æ¡")
    if progress["failed"]:
        print(f"âš  å†å²å¤±è´¥: {len(progress['failed'])} æ¡ï¼ˆæœ¬æ¬¡ä¼šç»§ç»­é‡è¯•ï¼‰")

    pending = [(i, r) for i, r in enumerate(records) if r["è®°å½•ID"] not in completed_ids]
    print(f"â³ å¾…å¤„ç†: {len(pending)} æ¡")

    if not pending:
        print("âœ… å…¨éƒ¨å·²å®Œæˆ!")
        save_csv(sort_details(all_details))
        return

    # è·å– Cookie
    print("\nğŸ”‘ è·å–éªŒè¯ Cookie...")
    cookies = acquire_cookies()

    # å¼‚æ­¥æŠ“å–
    print(f"\nğŸš€ å¼€å§‹å¼‚æ­¥æŠ“å– (å¹¶å‘æ•°: {MAX_CONCURRENT})...")
    start_time = time.time()

    errors = asyncio.run(
        scrape_batch(pending, cookies, all_details, completed_ids, progress)
    )

    elapsed = time.time() - start_time

    # æœ€ç»ˆä¿å­˜
    all_details = sort_details(all_details)
    progress["details"] = all_details
    save_progress(progress)

    with open(DETAILS_JSON, "w", encoding="utf-8") as f:
        json.dump(all_details, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ JSON â†’ {DETAILS_JSON}")

    save_csv(all_details)

    # ç»Ÿè®¡
    cats = {}
    for c in all_details:
        k = c.get("ä¸šåŠ¡ç±»å‹") or "æœªçŸ¥"
        cats[k] = cats.get(k, 0) + 1
    print("\nğŸ“Š ä¸šåŠ¡ç±»å‹åˆ†å¸ƒ:")
    for k, v in sorted(cats.items(), key=lambda x: -x[1]):
        print(f"    {k}: {v} æ¡")

    print(f"\n{'=' * 60}")
    print(f"âœ… ç¬¬äºŒæ­¥å®Œæˆ! å…± {len(all_details)} æ¡è¯¦æƒ…")
    print(f"â±  è€—æ—¶: {elapsed:.1f}s | å¹³å‡: {elapsed / max(len(pending), 1):.2f}s/æ¡")
    if errors:
        print(f"âš   æœ¬è½®å¤±è´¥ {errors} æ¡ï¼ˆå¯é‡æ–°è¿è¡Œè‡ªåŠ¨é‡è¯•ï¼‰")
    if progress["failed"]:
        print(f"âš   ç´¯è®¡å¾…é‡è¯• {len(progress['failed'])} æ¡")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    main()
