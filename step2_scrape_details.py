"""
ç¬¬äºŒæ­¥ï¼šæŠ“å–è¯¦æƒ…é¡µæ•°æ®
========================
æœ¬è„šæœ¬è¯»å– `output/links.json` ä¸­çš„é“¾æ¥åˆ—è¡¨ï¼Œé€ä¸ªè®¿é—®è¯¦æƒ…é¡µï¼Œ
æå–é¡µé¢ä¸­çš„ç»“æ„åŒ–æ•°æ®ï¼ˆè¡¨æ ¼ã€é”®å€¼å¯¹ã€æ­£æ–‡ç­‰ï¼‰ã€‚

ç”¨æ³•:
    python step2_scrape_details.py

åŠŸèƒ½ç‰¹æ€§:
    1. **æ–­ç‚¹ç»­ä¼ **: è‡ªåŠ¨è®°å½•å·²æŠ“å–çš„è®°å½• ID åˆ° `output/progress.json`ã€‚
       å¦‚æœè„šæœ¬ä¸­æ–­ï¼Œå†æ¬¡è¿è¡Œä¼šè‡ªåŠ¨è·³è¿‡å·²å®Œæˆçš„è®°å½•ã€‚
    2. **è‡ªåŠ¨é‡è¯•**: é‡åˆ°ç½‘ç»œé”™è¯¯æˆ–åçˆ¬éªŒè¯æ—¶ä¼šè‡ªåŠ¨é‡è¯•ã€‚
    3. **JSL ç»•è¿‡**: å¯åŠ¨æ—¶è‡ªåŠ¨é€šè¿‡ JSL éªŒè¯ã€‚

è¾“å‡º:
    - output/details.csv (Excel å¯æ‰“å¼€)
    - output/details.json (åŸå§‹æ•°æ®)
"""

import json
import csv
import os
import re
import time
from playwright.sync_api import sync_playwright

# ============ é…ç½® ============
PAGE_URL = "https://www.cqggzy.com/jyxx/transaction_detail.html"
INPUT_FILE = "output/links.json"
OUTPUT_CSV = "output/details.csv"
OUTPUT_JSON = "output/details.json"
PROGRESS_FILE = "output/progress.json"
BATCH_SIZE = 10  # æ¯ N æ¡ä¿å­˜ä¸€æ¬¡è¿›åº¦
# ==============================


def load_progress() -> dict:
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"completed": [], "details": []}


def save_progress(progress: dict):
    with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)


def extract_detail_data(page) -> dict:
    """ä»è¯¦æƒ…é¡µ DOM æå–æ‰€æœ‰ç»“æ„åŒ–æ•°æ®"""
    return page.evaluate(r"""
    () => {
        const result = {};

        // 1. é¡µé¢æ ‡é¢˜
        const h2 = document.querySelector('.detail-title, .article-title, h2, h3');
        if (h2) result['é¡µé¢æ ‡é¢˜'] = h2.textContent.trim();

        // 2. é¡¹ç›®ç¼–å·
        const projNum = document.querySelector('.detail-code, .project-code');
        if (projNum) result['é¡¹ç›®ç¼–å·'] = projNum.textContent.replace(/é¡¹ç›®ç¼–å·[ï¼š:]\s*/, '').trim();

        // 3. ä¿¡æ¯æ—¶é—´
        const bodyText = document.body.innerText;
        const timeMatch = bodyText.match(/ã€ä¿¡æ¯æ—¶é—´[ï¼š:]?\s*(\d{4}[-/]\d{2}[-/]\d{2})/);
        if (timeMatch) result['ä¿¡æ¯æ—¶é—´'] = timeMatch[1];

        // 4. è¡¨æ ¼ key-value
        document.querySelectorAll('table tr').forEach(tr => {
            const cells = Array.from(tr.querySelectorAll('td, th'));
            if (cells.length === 2) {
                const key = cells[0].textContent.trim().replace(/[ï¼š:]/g, '');
                const val = cells[1].textContent.trim();
                if (key && key.length < 30) result[key] = val;
            } else if (cells.length >= 4 && cells.length % 2 === 0) {
                for (let i = 0; i < cells.length; i += 2) {
                    const key = cells[i].textContent.trim().replace(/[ï¼š:]/g, '');
                    const val = cells[i+1].textContent.trim();
                    if (key && key.length < 30 && val) result[key] = val;
                }
            }
        });

        // 5. æ­£æ–‡ "ä¸€ã€xxxï¼šyyy" æ ¼å¼
        const contentEl = document.querySelector('.ewb-article-info, .article-content, .detail-content, .content-box');
        if (contentEl) {
            const text = contentEl.innerText;
            const kvPattern = /[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ã€.ï¼]\s*([^ï¼š:]+)[ï¼š:]\s*([^\n]+)/g;
            let m;
            while ((m = kvPattern.exec(text)) !== null) {
                const key = m[1].trim();
                const val = m[2].trim();
                if (key.length < 30 && val.length < 500 && !result[key]) result[key] = val;
            }
        }

        // 6. æ­£æ–‡å…¨æ–‡
        const main = contentEl || document.querySelector('.ewb-article, .article, .main-content, main');
        if (main) result['æ­£æ–‡å†…å®¹'] = main.innerText.trim().substring(0, 3000);

        return result;
    }
    """)


def save_csv(details: list[dict]):
    """ä¿å­˜ä¸º UTF-8 BOM ç¼–ç çš„ CSV"""
    if not details:
        return
    base = ["åºå·", "æ ‡é¢˜", "å‘å¸ƒæ—¥æœŸ", "ä¸šåŠ¡ç±»å‹", "åŒºåŸŸ", "è¯¦æƒ…é“¾æ¥"]
    extra = set()
    for d in details:
        for k in d.keys():
            if k not in base:
                extra.add(k)
    extra_sorted = sorted(extra - {"æ­£æ–‡å†…å®¹", "é”™è¯¯"})
    if "é”™è¯¯" in extra:
        extra_sorted.append("é”™è¯¯")
    if "æ­£æ–‡å†…å®¹" in extra:
        extra_sorted.append("æ­£æ–‡å†…å®¹")
    all_fields = base + extra_sorted

    with open(OUTPUT_CSV, "w", encoding="utf-8-sig", newline="") as f:
        w = csv.DictWriter(f, fieldnames=all_fields, extrasaction="ignore")
        w.writeheader()
        w.writerows(details)
    print(f"ğŸ’¾ CSV â†’ {OUTPUT_CSV} ({len(details)} æ¡, {len(all_fields)} åˆ—)")


def main():
    print("=" * 60)
    print("ğŸ“„ ç¬¬äºŒæ­¥ï¼šæŠ“å–è¯¦æƒ…é¡µæ•°æ®")
    print("=" * 60)

    if not os.path.exists(INPUT_FILE):
        print(f"âŒ æœªæ‰¾åˆ° {INPUT_FILE}ï¼Œè¯·å…ˆè¿è¡Œ step1_fetch_links.py")
        return

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        records = json.load(f)
    print(f"ğŸ“„ åŠ è½½ {len(records)} æ¡é“¾æ¥")

    progress = load_progress()
    completed_ids = set(progress["completed"])
    all_details = progress["details"]
    print(f"ğŸ“Š å·²å®Œæˆ: {len(completed_ids)} æ¡")

    pending = [(i, r) for i, r in enumerate(records) if r["è®°å½•ID"] not in completed_ids]
    print(f"â³ å¾…å¤„ç†: {len(pending)} æ¡")

    if not pending:
        print("âœ… å…¨éƒ¨å·²å®Œæˆ!")
        save_csv(all_details)
        return

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=False,
            args=["--disable-blink-features=AutomationControlled"],
        )
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            locale="zh-CN",
            viewport={"width": 1280, "height": 900},
        )
        page = context.new_page()

        # é€šè¿‡ JSL éªŒè¯
        print("\nğŸ”‘ é€šè¿‡ JSL åçˆ¬éªŒè¯...")
        page.goto(PAGE_URL, wait_until="commit", timeout=30000)
        for i in range(15):
            time.sleep(2)
            try:
                title = page.title()
                if "å…¬å…±èµ„æº" in title or "äº¤æ˜“" in title:
                    print(f"  âœ… éªŒè¯é€šè¿‡! ({(i + 1) * 2}s)")
                    break
            except Exception:
                continue
        else:
            print("  âš  éªŒè¯è¶…æ—¶, ç»§ç»­...")
        time.sleep(2)

        # é€æ¡æŠ“å–
        error_count = 0
        for idx, (orig_idx, record) in enumerate(pending):
            rid = record["è®°å½•ID"]
            title = record["æ ‡é¢˜"][:40]
            url = record["è¯¦æƒ…é“¾æ¥"]
            pct = (orig_idx + 1) / len(records) * 100

            print(f"\n[{orig_idx + 1}/{len(records)}] ({pct:.1f}%) {title}...")

            try:
                page.goto(url, wait_until="domcontentloaded", timeout=20000)
                time.sleep(1.5)
                detail = extract_detail_data(page)

                merged = {
                    "åºå·": len(completed_ids) + idx + 1,
                    "æ ‡é¢˜": record["æ ‡é¢˜"],
                    "å‘å¸ƒæ—¥æœŸ": record["å‘å¸ƒæ—¥æœŸ"],
                    "ä¸šåŠ¡ç±»å‹": record["ä¸šåŠ¡ç±»å‹"],
                    "åŒºåŸŸ": record["åŒºåŸŸ"],
                    "è¯¦æƒ…é“¾æ¥": url,
                }
                merged.update(detail)
                all_details.append(merged)
                completed_ids.add(rid)
                progress["completed"].append(rid)
                error_count = 0

                keys = [k for k in detail if k != "æ­£æ–‡å†…å®¹"]
                print(f"  âœ… {len(keys)} ä¸ªå­—æ®µ: {', '.join(keys[:5])}")

            except Exception as e:
                error_count += 1
                print(f"  âŒ {e}")
                all_details.append({
                    "åºå·": len(completed_ids) + idx + 1,
                    "æ ‡é¢˜": record["æ ‡é¢˜"],
                    "å‘å¸ƒæ—¥æœŸ": record["å‘å¸ƒæ—¥æœŸ"],
                    "ä¸šåŠ¡ç±»å‹": record["ä¸šåŠ¡ç±»å‹"],
                    "åŒºåŸŸ": record["åŒºåŸŸ"],
                    "è¯¦æƒ…é“¾æ¥": url,
                    "é”™è¯¯": str(e),
                })
                completed_ids.add(rid)
                progress["completed"].append(rid)

                if error_count >= 5:
                    print("  âš  è¿ç»­é”™è¯¯, é‡æ–°éªŒè¯...")
                    try:
                        page.goto(PAGE_URL, wait_until="commit", timeout=30000)
                        time.sleep(5)
                        error_count = 0
                    except Exception:
                        pass

            if (idx + 1) % BATCH_SIZE == 0:
                progress["details"] = all_details
                save_progress(progress)
                print(f"  ğŸ’¾ è¿›åº¦å·²ä¿å­˜ ({len(completed_ids)} æ¡)")

            time.sleep(0.5)

        browser.close()

    # æœ€ç»ˆä¿å­˜
    progress["details"] = all_details
    save_progress(progress)

    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(all_details, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ JSON â†’ {OUTPUT_JSON}")

    save_csv(all_details)

    cats = {}
    for c in all_details:
        k = c.get("ä¸šåŠ¡ç±»å‹") or "æœªçŸ¥"
        cats[k] = cats.get(k, 0) + 1
    print(f"\nğŸ“Š ä¸šåŠ¡ç±»å‹åˆ†å¸ƒ:")
    for k, v in sorted(cats.items(), key=lambda x: -x[1]):
        print(f"    {k}: {v} æ¡")

    print(f"\n{'=' * 60}")
    print(f"âœ… ç¬¬äºŒæ­¥å®Œæˆ! å…± {len(all_details)} æ¡è¯¦æƒ…")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    main()
